{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7igW5_CFE1r"
   },
   "source": [
    "# Numpy implementation of logistic regression with spike-slab prior\n",
    "\n",
    "This is a hand-coding version of the logistc regression with spike-slab prior, based on Numpy. This serves as a double check of the result based on PyMC3 from [this project](https://drive.google.com/file/d/161KAaWM-ur6PaqfhNUqpoJMePu8-EosA/view). In that notebook, the biological background has been abstracted out and provides codes with PyMC3 and PyMC4  with different results. In this notebook, we provide a hand-coding version whose model is consistent with the settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4x17zZqTFE1t"
   },
   "source": [
    "## Statistical models without biological background\n",
    "\n",
    "We simulated a data-set containing 1,000 samples with observed binary response ($y \\in \\{0,1\\}$), under logistic regression $\\text{logit}(E(y)) = \\alpha + X\\beta$ where $X$ has 3 variables $x_1 \\ne x_2 = x_3$, with effect $\\beta_1 = 0, \\beta_2 \\ne 0, \\beta_3 = 0$.\n",
    "\n",
    "We model variable $j$'s effect using a spike slab prior, $\\beta_j \\sim \\pi_{j} \\delta_0 + (1 - \\pi_{j}) N (\\mu, \\sigma^2)$. Due to simulation settings we expect posterior of $\\pi$ is close to 0 for $x_1$. Since  $x_2$ and $x_3$ are identical, we expect posterior of $\\pi$ also identical for $x_2$ and $x_3$, and are both 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rVGfQL8FE1u"
   },
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmBslJfgFE1u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def simulate_logistic(x, b):\n",
    "    z = x.dot(b)\n",
    "    p = 1/(1+np.exp(-z))\n",
    "    return np.random.binomial(1., p).astype(np.float32)\n",
    "\n",
    "def get_X(n,p,seed=999):\n",
    "    np.random.seed(seed)\n",
    "    x1 = np.random.binomial(1,p,n)\n",
    "    x2 = np.random.binomial(1,p,n)\n",
    "    x3 = x2\n",
    "    return np.vstack([x1, x2, x3]).T.astype(np.float32)\n",
    "\n",
    "seed = 999\n",
    "b = [0,1,0]\n",
    "X = get_X(1000,0.2,seed)\n",
    "y = simulate_logistic(X,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDSrquSaFE10",
    "outputId": "a052eb49-9da6-4a58-f2c1-c66a40bd38ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgR-20QzFE15",
    "outputId": "a2884945-2ecf-4127-8797-9e4157f60423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKYIVQUSFE19"
   },
   "source": [
    "### Some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7TXTCA7FE1-"
   },
   "outputs": [],
   "source": [
    "iteration = 2000\n",
    "tune_prop = 0.1\n",
    "n_chain = 3\n",
    "n_thread = 4\n",
    "\n",
    "pi0 = 0.051366009925488\n",
    "mu = 0.783230896500752\n",
    "sigma = 0.816999481742865\n",
    "lower = -2.94\n",
    "upper = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOLglaaHFE2H"
   },
   "source": [
    "## PyMC3 spike-slab logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhsTAMLlFE2H",
    "outputId": "764cd0e6-563a-4068-d244-b772fe06f28a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_same_graph' from 'theano.gof.toolbox' (/Users/jungang/anaconda3/lib/python3.8/site-packages/theano/gof/toolbox.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f3a21f82a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbvsr_pm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minvlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymc3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0m__set_compiler_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymc3/__init__.py\u001b[0m in \u001b[0;36m__set_compiler_flags\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__set_compiler_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Workarounds for Theano compiler problems on various platforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcxxflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m     object2, utils)\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m from theano.compile import (\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mSymbolicInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mSymbolicOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/theano/compile/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_dump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/theano/compile/function/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpfunc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0morig_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/theano/compile/function/pfunc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnusedInputError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileStats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/theano/compile/function/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops_with_inner_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_same_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_same_graph' from 'theano.gof.toolbox' (/Users/jungang/anaconda3/lib/python3.8/site-packages/theano/gof/toolbox.py)"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm3\n",
    "import theano.tensor as tt\n",
    "\n",
    "def bvsr_pm3(y, X, pi0=0.5, mu=0, sigma=1, lower=-1, upper=1):\n",
    "    invlogit = lambda x: 1/(1 + tt.exp(-x))\n",
    "    model = pm3.Model()\n",
    "    with model:\n",
    "        xi = pm3.Bernoulli('xi', pi0, shape = X.shape[1])\n",
    "        beta_offset = pm3.Normal('beta_offset', mu = 0, sd = 1, shape = X.shape[1])\n",
    "        beta = pm3.Deterministic(\"beta\", mu + beta_offset * sigma)\n",
    "        alpha_offset = pm3.Uniform(\"alpha_offset\", lower = -1, upper = 1)\n",
    "        alpha = pm3.Deterministic(\"alpha\", lower + (alpha_offset+1)/2*(upper - lower))\n",
    "        p = pm3.math.dot(X, xi * beta)\n",
    "        y_obs = pm3.Bernoulli('y_obs', invlogit(p + alpha), observed = y)\n",
    "    return model\n",
    "\n",
    "m3 = bvsr_pm3(y,X, pi0, mu, sigma, lower, upper)\n",
    "target_accept = 0.95\n",
    "\n",
    "with m3:\n",
    "    trace3 = pm3.sample(draws = iteration, random_seed = seed, cores = n_thread, progressbar = True, chains = n_chain, tune = int(tune_prop*iteration), nuts = {\"target_accept\": target_accept})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0CeTjt7FE2c"
   },
   "source": [
    "## PyMC3 result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vz3_2C7WFE2e"
   },
   "source": [
    "### Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG50PDM3FE2f",
    "outputId": "8675265f-f7fe-47cd-b136-4a5f8abf659e"
   },
   "outputs": [],
   "source": [
    "status = pm3.traceplot(trace3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lf9NSd3lFE2i"
   },
   "source": [
    "### Posterior estimate of $\\pi$\n",
    "\n",
    "We expect them to be (0, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQ2iuCDyFE2j",
    "outputId": "fc52a845-8acc-4f67-8b16-98b59e59f970"
   },
   "outputs": [],
   "source": [
    "np.apply_along_axis(np.mean, 0, trace3['xi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fj6sJCAAFE2n"
   },
   "source": [
    "### Posterior estimate of $\\alpha$\n",
    "We expect it be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubUXLmGoFE2o",
    "outputId": "0f863055-8187-496b-caa5-060583b124ae"
   },
   "outputs": [],
   "source": [
    "np.mean(trace3['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-writing HMC-MH Block Sampler of Posterior Distribution\n",
    "\n",
    "#### In this part, we rename variable $\\pi$ as g, to avoid some inconvenience in coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we provided a hand-writing version of Hamiltonian Monte Carlo sampling to the posterior distribution. As we know, HMC is well-known monte carlo method for high-dimensional data. The basic idea is borrowed from Hamilton System, which is a classical dynamic system in physics. A brief introduction of HMC can be found [here](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo).\n",
    "\n",
    "Followed by the instruction of the steps of HMC in [Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/), we need to calculate the derivatives of log-posterior distribution of each parameters and samples based on the idea of Hamiltonian System. After a series of sampling, a accept-reject process are proposed to decide the quality of sampling.\n",
    "\n",
    "However, although HMC is a rather powerful and efficient tool to sample, it can be only applied to continuous variables. In our setting, variable $g$ is a binary variable, which cannot be sampled by HMC. So, we use HMC to sample continuous variables and Metropolis-Hastings to sample binary variable. These 2 parts consist of the whole HMC-MH block sampler.\n",
    "\n",
    "\n",
    "#### Likelihood: \n",
    "\n",
    "$y \\sim Bernoulli( sigmoid (X(\\beta \\odot g)  + \\alpha ))$,   where $sigmoid(x)=\\frac{1}{1 + e^{-x}}$,   $\\odot$ means element-wise product\n",
    "\n",
    "\n",
    "#### Prior: \n",
    "\n",
    "$g \\sim \\prod Bernoulli(pi0)$\n",
    "\n",
    "$\\beta \\sim \\prod Normal(\\mu, \\sigma^2)$\n",
    "\n",
    "$\\alpha \\sim Uniform(lower + (upper - lower) * \\frac{lower + 1}{2}, lower + (upper - lower) * \\frac{upper + 1}{2})$\n",
    "\n",
    "where pi0, $\\mu$, $\\sigma^2$, lower, upper are hyperparameters.\n",
    "\n",
    "\n",
    "#### Joint Posterior:\n",
    "\n",
    "$p(g,\\alpha,\\beta|y,x) \\varpropto p(y|g, \\beta, \\alpha, x)p(\\alpha|x) p(\\beta | x) p(g | x) =  \\prod^n \\{(sigmoid(X_i(\\beta \\odot g) + \\alpha))^{y_i}(1 - sigmoid(X_i(\\beta \\odot g) + \\alpha))^{1 - y_i} \\} * exp(-\\frac{(\\beta - \\mu)^{'}{(\\beta - \\mu)}}{2 \\sigma^2}) * \\prod (pi0)^{g} * (1 - pi0)^{1 - g} *  I\\{\\alpha \\in (lower + (upper - lower) * \\frac{1}{2}, lower + (upper - lower))\\}$\n",
    "\n",
    "\n",
    "##### Log-posterior partial derivative:\n",
    "\n",
    "$\\frac{\\partial{logp}}{\\partial g_j} = \\sum_i^n \\{ y_i X_{ij} \\beta_{j} - sigmoid(x_i (\\beta \\odot g) + \\alpha) * x_{ij} \\beta_j \\}  + log(\\frac{pi0}{1 - pi0})$,  (this derivative of discrete variable cannot be used in HMC)\n",
    "\n",
    "$\\frac{\\partial{logp}}{\\partial \\alpha} = \\sum_i^n \\{ sigmoid(-x_i(\\beta \\odot g) - \\alpha) - (1- y_i) \\}$, where $\\alpha \\in (lower + (upper - lower) * \\frac{1}{2}, lower + (upper - lower))$\n",
    "\n",
    "$\\frac{\\partial{logp}}{\\partial \\beta_j} = \\sum_i^n \\{ sigmoid(-x_i(\\beta \\odot g) - \\alpha) * x_{ij} g_j + (1-y_i)(-x_{ij} g_j) \\} - \\frac{(\\beta_j - \\mu)}{\\sigma^2} $\n",
    "\n",
    "After carefully calculate the partial derivative of each parameter for log-posterior density, basic HMC sampling algorithm will be exploited to sample $\\alpha$, $\\beta$.\n",
    "\n",
    "Since the $\\alpha$ need to be in an accept region with constrain, we simply use the suggestion from [Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/), where we reverse the direction of momentum variable when $\\alpha$ reaches the rejected region.\n",
    "\n",
    "\n",
    "##### Update for $g_i$ :\n",
    "Metropolis-Hastings algorithm will be applied to sample $g_i$, since HMC can only be used for continuous variables. In this sense, random sample $gi_{new} \\sim Bernoulli(0.5)$ is the transition kernel, where all the transition probabilities from $gi_{old}$ to $gi_{new}$ are equal, to reduce some computations of M-H algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the log-posterior partial derivative functions and log-posterior density functions\n",
    "\n",
    "sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# the log unnormalized posterior probability of all parameters\n",
    "def log_posterior(X, y, g, beta, alpha, mu, sigma2, pi0):\n",
    "    logp = (y * np.log(sigmoid(np.dot(X, beta*g) + alpha)) + (1 - y) * np.log(1 - sigmoid(np.dot(X, (beta * g)) + alpha))).sum() - (beta - mu).T.dot(beta - mu) / (2 * sigma2) + g.sum() * np.log(pi0) + np.log(1 - pi0) * (g.size - g.sum())\n",
    "    return logp\n",
    "\n",
    "# the partial derivative of log unnormalized posterior probability of g_j\n",
    "def dlogp_dgj(X, y, g, beta, alpha, pi0, j):\n",
    "    pd = (y * X[:, j] * beta[j] - sigmoid(X.dot(beta * g) + alpha) * X[:, j] * beta[j]).sum() + np.log(pi0 / (1 - pi0))\n",
    "    return pd\n",
    "\n",
    "# the partial derivative of log unnormalized posterior probability of alpha\n",
    "def dlogp_dalpha(X, y, g, beta, alpha):\n",
    "    pd = y - sigmoid(np.dot(X, beta * g) + alpha)\n",
    "    return pd.sum()\n",
    "\n",
    "# the partial derivative of log unnormalized posterior probability of beta_j\n",
    "def dlogp_dbetaj(X, y, g, beta, alpha, mu, sigma2, j):\n",
    "    pd = (y * X[:, j] * g[j] - sigmoid(X.dot(beta * g) + alpha) * X[:, j] * g[j]).sum() - (beta[j] - mu) / sigma2\n",
    "    return pd\n",
    "\n",
    "# the log unnormalized marginal posterior probability of g_j\n",
    "def marginal_logpost_g(X, y, g, beta, alpha, pi0, j):\n",
    "    pg = (y * np.log(sigmoid(X.dot(beta * g) + alpha)) + (1 - y) * np.log(1 - sigmoid(X.dot(beta * g) + alpha))).sum() + np.log(pi0) * g[j] + np.log(1 - pi0) * g[j]\n",
    "    return pg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling algorithm\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def alpha_in_region(lower, upper, alpha):\n",
    "    # check whether the sampled alphas are in the accepted region\n",
    "    l = lower + (upper - lower) * (0 + 1) / 2\n",
    "    r = lower + (upper - lower) * (1 + 1) / 2\n",
    "    if alpha >= l and alpha <= r:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def update_g(X, y, g, beta, alpha, pi0, accept_n):\n",
    "    # update g by Metropolis-Hasting Algorithm\n",
    "    g_new = np.random.binomial(n = 1, p = 0.5, size = g.size)\n",
    "    log_r = 0\n",
    "    for k in range(X.shape[1]):\n",
    "        log_r += marginal_logpost_g(X, y, g_new, beta, alpha, pi0, k)\n",
    "        log_r -= marginal_logpost_g(X, y, g, beta, alpha, pi0, k)\n",
    "    if log_r > np.log(np.random.rand(1)[0]):\n",
    "        accept_n += 1\n",
    "        return g_new, accept_n\n",
    "    else:\n",
    "        return g, accept_n\n",
    "    \n",
    "def update_HMC(X, y, g, beta, alpha, pi0, L, e, M, M_1, mu, sigma, lower, upper, accept_n):\n",
    "    # update beta and alpha by Hamiltonian Monte Carlo Algorithm\n",
    "    phi = np.random.multivariate_normal(np.zeros(beta.size + alpha.size), M)\n",
    "    beta_new = beta\n",
    "    alpha_new = alpha\n",
    "    phi_new = phi.copy()   \n",
    "    \n",
    "    # leafrog\n",
    "    for i in range(L):\n",
    "        # half step   update momentum\n",
    "        dlogp_beta = []\n",
    "        dlogp_alpha = [dlogp_dalpha(X, y, g, beta, alpha)]\n",
    "        for k in range(X.shape[1]):\n",
    "            dlogp_beta.append(dlogp_dbetaj(X, y, g, beta, alpha, mu, sigma**2, k))\n",
    "        dlogp = np.concatenate([dlogp_beta, dlogp_alpha])\n",
    "        phi_new = phi_new + 0.5 * e * dlogp\n",
    "        \n",
    "        # update parameter\n",
    "        p = np.concatenate((beta_new, alpha_new))\n",
    "        p = p + e * M_1.dot(phi_new)\n",
    "        beta_new = p[:beta.size]\n",
    "        alpha_new = np.array([p[beta.size]])\n",
    "        \n",
    "        \n",
    "        # half step   update momentum\n",
    "        dlogp_beta = []\n",
    "        dlogp_alpha = [dlogp_dalpha(X, y, g, beta_new, alpha_new)]\n",
    "        for k in range(X.shape[1]):\n",
    "            dlogp_beta.append(dlogp_dbetaj(X, y, g, beta_new, alpha_new, mu, sigma**2, k))\n",
    "        dlogp = np.concatenate([dlogp_beta, dlogp_alpha])\n",
    "        phi_new = phi_new + 0.5 * e * dlogp\n",
    "        \n",
    "        if not alpha_in_region(lower, upper, alpha_new):\n",
    "            # if alpha is not in the accepted region, then reverse the direction of momentum\n",
    "            phi_new *= -1\n",
    "    \n",
    "    # accept or reject\n",
    "    log_r = log_posterior(X, y, g, beta_new, alpha_new, mu, sigma ** 2, pi0) - log_posterior(X, y, g, beta, alpha, mu, sigma ** 2, pi0) + (phi_new - mu).T.dot(M_1).dot(phi_new - mu) - (phi - mu).T.dot(M_1).dot(phi - mu)  \n",
    "    if log_r > np.log(np.random.rand(1)[0]) and alpha_in_region(lower, upper, alpha_new):\n",
    "        accept_n += 1\n",
    "        return beta_new, alpha_new, accept_n\n",
    "    else:\n",
    "        return beta, alpha, accept_n\n",
    "\n",
    "\n",
    "def MCMC(burnings, samples, sampling_gap, X, y, g, beta, alpha, pi0, mu, sigma, lower, upper, L = 10, e = 0.1, M_scale = 1, ith_iteration = 0):\n",
    "    #M = np.diag(np.ones(beta.size + alpha.size)) * M_scale\n",
    "    \n",
    "    # set the hyperparameters for HMC\n",
    "    \n",
    "    \n",
    "    # carefully design the Mass Matrix for the sampling of momentums, which is the estimated inverse-covariance matrix for joint posterior distribution\n",
    "    M = np.array(\n",
    "    [\n",
    "     [0.01, 0, 0, 0],\n",
    "     [0, 0.01, 0, 0],\n",
    "     [0, 0, 0.01, 0],\n",
    "     [0, 0, 0, 20]\n",
    "    ]\n",
    "    )\n",
    "    M_1 = np.linalg.inv(M)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # burning phrase\n",
    "    \n",
    "    HMC_accept_n = 0\n",
    "    MH_accept_n = 0\n",
    "    \n",
    "    for i in range(burnings):\n",
    "        g, MH_accept_n = update_g(X, y, g, beta, alpha, pi0, MH_accept_n)\n",
    "        beta, alpha, HMC_accept_n = update_HMC(X, y, g, beta, alpha, pi0, L, e, M, M_1, mu, sigma, lower, upper, HMC_accept_n)\n",
    "    print(str(ith_iteration)+ \"-th iteration:\", \"MH accept rate in burning phrase:\", MH_accept_n / burnings)\n",
    "    print(str(ith_iteration)+ \"-th iteration:\", \"HMC accept rate in burning phrase:\", HMC_accept_n / burnings)\n",
    "        \n",
    "        \n",
    "    # sampling phrase\n",
    "    g_samples = []\n",
    "    beta_samples = []\n",
    "    alpha_samples = []\n",
    "    HMC_accept_n = 0\n",
    "    MH_accept_n = 0\n",
    "    \n",
    "    for i in range(samples * sampling_gap):\n",
    "        g, MH_accept_n = update_g(X, y, g, beta, alpha, pi0, MH_accept_n)\n",
    "        beta, alpha, HMC_accept_n = update_HMC(X, y, g, beta, alpha, pi0, L, e, M, M_1, mu, sigma, lower, upper, HMC_accept_n)  \n",
    "        if i % sampling_gap == 0:\n",
    "            # sampling each sample by a gap\n",
    "            g_samples.append(g)\n",
    "            beta_samples.append(beta)\n",
    "            alpha_samples.append(alpha)\n",
    "    print(str(ith_iteration)+ \"-th iteration:\", \"MH accept rate in sampling phrase:\", MH_accept_n / (samples * sampling_gap))\n",
    "    print(str(ith_iteration)+ \"-th iteration:\", \"HMC accept rate in sampling phrase:\", HMC_accept_n / (samples * sampling_gap))\n",
    "    \n",
    "    return np.stack(g_samples), np.stack(beta_samples), np.stack(alpha_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "def Parallel_MCMC(n_parallel, burnings, samples, sampling_gap, X, y, pi0, mu, sigma, lower, upper, L = 10, e = 0.1, M_scale = 1):\n",
    "    # this is the function for multiple chains to sample\n",
    "    parameters = []\n",
    "    for i in range(n_parallel):\n",
    "        g = np.random.binomial(1, 0.5, X.shape[1])\n",
    "        beta = np.random.normal(0, 1, X.shape[1])\n",
    "        alpha = np.random.uniform(lower + (upper - lower) * (1) / 2, lower + (upper - lower) * (1 + 1) / 2, [1])\n",
    "        print(g, beta, alpha)\n",
    "        parameters.append([burnings, samples, sampling_gap, X.copy(), y.copy(), g.copy(), beta.copy(), alpha.copy(), pi0, mu, sigma, lower, upper, L, e, M_scale, i])\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(MCMC)(i[0], i[1], i[2], i[3], i[4], i[5], i[6], i[7], i[8], i[9], i[10], i[11], i[12], i[13], i[14], i[15], i[16] ) for i in parameters)\n",
    "    return results\n",
    "\n",
    "def MCMC2(burnings, samples, sampling_gap, X, y, g, beta, alpha, pi0, mu, sigma, lower, upper, L = 10, e = 0.1, M_scale = 1, ith_iteration = 0):\n",
    "    return burnings, samples, sampling_gap, X, y, g, beta, alpha, pi0, mu, sigma, lower, upper, L, e , M_scale, ith_iteration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "L = 100\n",
    "e = 0.01\n",
    "M_scale = 1\n",
    "\n",
    "g = np.random.binomial(1, 0.5, X.shape[1])\n",
    "beta = np.random.normal(0, 1, X.shape[1])\n",
    "alpha = np.random.uniform(lower + (upper - lower) * (1) / 2, lower + (upper - lower) * (1 + 1) / 2, [1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start to sample for only 1 chain\n",
    "g_samples1, beta_samples1, alpha_samples1 = MCMC(1000, 1000, 2, X, y, g, beta, alpha, pi0, mu, sigma, lower, upper, L, e, M_scale)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample for 3 chains\n",
    "multiple_results = Parallel_MCMC(2, 1000, 1000, 2, X, y, pi0, mu, sigma, lower, upper, L = 10, e = 0.1, M_scale = 1)\n",
    "\n",
    "multiple_results\n",
    "#multiple_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_samples1, beta_samples1, alpha_samples1 = multiple_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "for i in range(g_samples1.shape[1]):\n",
    "    chain = g_samples1[:, i]\n",
    "    num, count = np.unique(chain, return_counts = True)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(121)\n",
    "    plt.bar(num, count)\n",
    "    plt.title('Density Plot for g' + str(i + 1))\n",
    "    plt.xticks([0, 1])\n",
    "    #plt.hist(chain, density = True, nbins = 100)\n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.arange(chain.shape[0]),chain)\n",
    "    plt.title('Trace Plot for g' + str(i + 1))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(beta_samples1.shape[1]):\n",
    "    chain = beta_samples1[:, i]\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(121)\n",
    "    sns.histplot(chain,  kde=True, stat = \"density\", binwidth= 0.1)\n",
    "    plt.title('Density Plot for $\\\\beta$' + str(i + 1))\n",
    "    plt.xlim(-5, 5)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.arange(chain.shape[0]),chain)\n",
    "    plt.title('Trace Plot for $\\\\beta$' + str(i + 1))\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain = alpha_samples1.reshape(-1)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(121)\n",
    "sns.histplot(chain,  kde=True, stat = \"density\", binwidth= 0.001)\n",
    "plt.title('Density Plot for $\\\\alpha$')\n",
    "#plt.xlim(-5, 5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(chain.shape[0]),chain)\n",
    "plt.title('Trace Plot for $\\\\alpha$')\n",
    "#plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Posterior estimate of  g\n",
    "\n",
    "We expect them to be (0, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(g_samples1).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Posterior estimate of $\\alpha$\n",
    "We expect it be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_samples1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From what has been showed above, the result of hand-writing version is consistent with the result from PyMC3. The posterior expectation of $\\tilde{g_1}$, $\\tilde{g_2}$, $\\tilde{g_3}$ are approximately 0, 0.5, 0.5. Also, for $\\tilde{\\alpha}$, the posterior estimator is very close to 0. These results are what we expected before simulation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "20200519_PyMC4_explore.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
